{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the imports\n",
    "import scipy as sp \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and sampling in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files which we have extracted from other notebooks\n",
    "citations_features = pd.read_parquet('./citations_features.parquet/', engine='pyarrow')\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the citations with their corresponding features\n",
    "dataset_with_features = pd.merge(\n",
    "    dataset, citations_features, how='inner', left_on=['id','citation'], right_on = ['id','citation']\n",
    ")\n",
    "dataset_with_features.drop('page_title_y', axis=1, inplace=True)\n",
    "dataset_with_features.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just considering the citations which are unique in nature so that are model cannot be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_features  = dataset_with_features.set_index(['id', 'citation'])\n",
    "dataset_with_features = dataset_with_features[~dataset_with_features.index.duplicated(keep='first')]\n",
    "dataset_with_features = dataset_with_features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 760,000 samples of scientific data\n",
    "scientific_data = dataset_with_features[dataset_with_features['scientific_or_not'] == True]\n",
    "scientific_samples = scientific_data.sample(n=760000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 760,000 samples of NON-scientific data\n",
    "non_scientific_data = dataset_with_features[dataset_with_features['scientific_or_not'] == False]\n",
    "non_scientific_samples = non_scientific_data.sample(n=760000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the scientific and non scientific samples and get unique 1.52 million data samples\n",
    "dataset_with_features = pd.concat([scientific_samples, non_scientific_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_features.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1520000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_with_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `sklearn` library to vectorize features such as `words` and `tags` and use the Bag of Words model and check for the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the vectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the required columns for words and tags\n",
    "shortened_time_features = dataset_with_features[\n",
    "    ['neighboring_words', 'neighboring_tags', 'citation', 'id', 'scientific_or_not']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing TAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a Random Forest takes a lot of time to train and construct the trees, we have decided to consider instead of the last 40 words just the last 20 words so that we don't have any memory problems. Also, this is because of the fact that the model is being trained locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1520000/1520000 [00:03<00:00, 448486.36it/s]\n",
      "100%|██████████| 1520000/1520000 [00:03<00:00, 413814.18it/s]\n"
     ]
    }
   ],
   "source": [
    "shortened_time_features['neighboring_words'] = shortened_time_features['neighboring_words'].progress_apply(\n",
    "    lambda x: x[-20:])\n",
    "shortened_time_features['neighboring_tags'] = shortened_time_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: x[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shortened tag counts for the last 20 words\n",
    "shortened_tag_counts = pd.Series(Counter(chain.from_iterable(x for x in shortened_time_features.neighboring_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "``       125\n",
       "WP$      951\n",
       "UH      1117\n",
       "SYM     1535\n",
       "PDT     1860\n",
       "$       5502\n",
       "EX      6100\n",
       "RBS     6117\n",
       "RBR    10730\n",
       "RP     12564\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_tag_counts.nsmallest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNP    7720415\n",
       "NN     7172442\n",
       "JJ     2825144\n",
       "CD     2709039\n",
       "IN     1763863\n",
       "NNS    1124781\n",
       ".      1095751\n",
       "DT      799749\n",
       ":       789523\n",
       "VB      618022\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_tag_counts.nlargest(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the `TAGs` which we think are not that useful such as `backquotes` and the `dollar` sing and replace it with the `Others` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1520000/1520000 [00:13<00:00, 111149.88it/s]\n"
     ]
    }
   ],
   "source": [
    "OTHER_TAGS = ['LS', '``', '$']\n",
    "shortened_time_features['neighboring_tags'] = shortened_time_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: [i if i not in OTHER_TAGS else 'Others' for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1520000/1520000 [00:03<00:00, 480622.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Making a string out of the tags since that is the format of the input for `CountVectorizer`\n",
    "shortened_time_features['neighboring_tags'] = shortened_time_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the tags and one hot encode them depending on if they are present in the list of neighboring tags for the corresponding citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_transformed_tags = cv.fit_transform(shortened_time_features['neighboring_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_transformed_tags = pd.DataFrame(shortened_transformed_tags.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results with the features\n",
    "shortened_time_features = pd.concat([shortened_time_features, shortened_transformed_tags], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing WORDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to process the `words` - because of resource constraints we have already decreased the number of number of neighboring words we are considering.\n",
    "\n",
    "Since the size of the vocabulary is too high - which is nearly half a million, it will create another memory constraint since the dataframe will be too big. So we use the Hash Vectorizer which hashes the one hot encoding and hence the issue of space is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1520000/1520000 [00:19<00:00, 78795.83it/s] \n"
     ]
    }
   ],
   "source": [
    "# Lowercasing all the neighboring words for each of the citation\n",
    "shortened_time_features['neighboring_words'] = shortened_time_features['neighboring_words'].progress_apply(\n",
    "    lambda x: [i.lower() for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1520000/1520000 [00:03<00:00, 395592.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now need to preprocess it to use Count Vectorizer from SKLearn\n",
    "shortened_time_features['neighboring_words'] = shortened_time_features['neighboring_words'].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Hash Vectorizer with the number of components for the vectors to be set as 500\n",
    "vectorizer = HashingVectorizer(n_features=500)\n",
    "corpus = shortened_time_features['neighboring_words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520000, 500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the corpus and we get a matrix of the size of the sample set with 500 components each\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making the training and testing sets, we take all the total indices of the sampled set and shuffle them.\n",
    "\n",
    "Then, we take 80% of it to be the training data and the other 20% to be the testing set. We get the corresponding `words/tags` features we create and stack/merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "TOTAL_SAMPLES = X.shape[0]\n",
    "indices = np.arange(TOTAL_SAMPLES)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the indices for training and testing sets\n",
    "training_end_index = int(((TOTAL_SAMPLES * 80) / 100))\n",
    "training_indices = indices[:training_end_index]\n",
    "testing_indices = indices[training_end_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for the corresponding indices from the features data\n",
    "y_train = shortened_time_features.loc[training_indices, 'scientific_or_not'].astype(int).tolist()\n",
    "y_test = shortened_time_features.loc[testing_indices, 'scientific_or_not'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tags for the corresponding set\n",
    "training_tags = shortened_transformed_tags.loc[training_indices, :].astype(int).values.tolist()\n",
    "testing_tags = shortened_transformed_tags.loc[testing_indices, :].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words for the set\n",
    "training_words = X[training_indices, :]\n",
    "testing_words = X[testing_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a `HashVectorizer` returns a `csr.matrix` which is Compressed Sparse Matrix as the output, we need to stack the columns together so that we have an accumualated dataset for the features for each citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = sp.sparse.hstack((training_tags, training_words))\n",
    "testing_features = sp.sparse.hstack((testing_tags, testing_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model time - just with 2 features `words` and `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "building tree 1 of 150\n",
      "building tree 2 of 150\n",
      "building tree 3 of 150[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  3.8min\n",
      "\n",
      "building tree 4 of 150[Parallel(n_jobs=2)]: Done   2 tasks      | elapsed:  3.9min\n",
      "\n",
      "building tree 5 of 150[Parallel(n_jobs=2)]: Done   3 tasks      | elapsed:  7.6min\n",
      "\n",
      "building tree 6 of 150[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:  7.7min\n",
      "\n",
      "building tree 7 of 150[Parallel(n_jobs=2)]: Done   5 tasks      | elapsed: 11.2min\n",
      "\n",
      "building tree 8 of 150[Parallel(n_jobs=2)]: Done   6 tasks      | elapsed: 11.3min\n",
      "\n",
      "building tree 9 of 150[Parallel(n_jobs=2)]: Done   7 tasks      | elapsed: 15.1min\n",
      "\n",
      "building tree 10 of 150[Parallel(n_jobs=2)]: Done   8 tasks      | elapsed: 15.3min\n",
      "\n",
      "building tree 11 of 150[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed: 18.6min\n",
      "\n",
      "building tree 12 of 150[Parallel(n_jobs=2)]: Done  10 tasks      | elapsed: 19.1min\n",
      "\n",
      "building tree 13 of 150[Parallel(n_jobs=2)]: Done  11 tasks      | elapsed: 22.4min\n",
      "\n",
      "building tree 14 of 150[Parallel(n_jobs=2)]: Done  12 tasks      | elapsed: 22.8min\n",
      "\n",
      "building tree 15 of 150[Parallel(n_jobs=2)]: Done  13 tasks      | elapsed: 26.3min\n",
      "\n",
      "building tree 16 of 150[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed: 26.4min\n",
      "\n",
      "building tree 17 of 150[Parallel(n_jobs=2)]: Done  15 tasks      | elapsed: 30.1min\n",
      "\n",
      "building tree 18 of 150[Parallel(n_jobs=2)]: Done  16 tasks      | elapsed: 30.1min\n",
      "\n",
      "building tree 19 of 150[Parallel(n_jobs=2)]: Done  17 tasks      | elapsed: 33.8min\n",
      "\n",
      "building tree 20 of 150[Parallel(n_jobs=2)]: Done  18 tasks      | elapsed: 34.0min\n",
      "\n",
      "building tree 21 of 150[Parallel(n_jobs=2)]: Done  19 tasks      | elapsed: 37.7min\n",
      "\n",
      "building tree 22 of 150[Parallel(n_jobs=2)]: Done  20 tasks      | elapsed: 37.7min\n",
      "\n",
      "building tree 23 of 150[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed: 41.3min\n",
      "\n",
      "building tree 24 of 150[Parallel(n_jobs=2)]: Done  22 tasks      | elapsed: 41.7min\n",
      "\n",
      "building tree 25 of 150[Parallel(n_jobs=2)]: Done  23 tasks      | elapsed: 45.1min\n",
      "\n",
      "building tree 26 of 150[Parallel(n_jobs=2)]: Done  24 tasks      | elapsed: 45.5min\n",
      "\n",
      "building tree 27 of 150[Parallel(n_jobs=2)]: Done  25 tasks      | elapsed: 49.0min\n",
      "\n",
      "building tree 28 of 150[Parallel(n_jobs=2)]: Done  26 tasks      | elapsed: 49.3min\n",
      "\n",
      "building tree 29 of 150[Parallel(n_jobs=2)]: Done  27 tasks      | elapsed: 53.0min\n",
      "\n",
      "building tree 30 of 150[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 53.1min\n",
      "\n",
      "building tree 31 of 150[Parallel(n_jobs=2)]: Done  29 tasks      | elapsed: 56.6min\n",
      "\n",
      "building tree 32 of 150[Parallel(n_jobs=2)]: Done  30 tasks      | elapsed: 56.8min\n",
      "\n",
      "building tree 33 of 150[Parallel(n_jobs=2)]: Done  31 tasks      | elapsed: 60.2min\n",
      "\n",
      "building tree 34 of 150[Parallel(n_jobs=2)]: Done  32 tasks      | elapsed: 60.3min\n",
      "\n",
      "building tree 35 of 150[Parallel(n_jobs=2)]: Done  33 tasks      | elapsed: 64.1min\n",
      "\n",
      "building tree 36 of 150[Parallel(n_jobs=2)]: Done  34 tasks      | elapsed: 64.2min\n",
      "\n",
      "building tree 37 of 150[Parallel(n_jobs=2)]: Done  35 tasks      | elapsed: 67.9min\n",
      "\n",
      "building tree 38 of 150[Parallel(n_jobs=2)]: Done  36 tasks      | elapsed: 67.9min\n",
      "\n",
      "building tree 39 of 150[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed: 71.4min\n",
      "\n",
      "building tree 40 of 150[Parallel(n_jobs=2)]: Done  38 tasks      | elapsed: 71.7min\n",
      "\n",
      "building tree 41 of 150[Parallel(n_jobs=2)]: Done  39 tasks      | elapsed: 75.1min\n",
      "\n",
      "building tree 42 of 150[Parallel(n_jobs=2)]: Done  40 tasks      | elapsed: 75.5min\n",
      "\n",
      "building tree 43 of 150[Parallel(n_jobs=2)]: Done  41 tasks      | elapsed: 78.6min\n",
      "\n",
      "building tree 44 of 150[Parallel(n_jobs=2)]: Done  42 tasks      | elapsed: 79.4min\n",
      "\n",
      "building tree 45 of 150[Parallel(n_jobs=2)]: Done  43 tasks      | elapsed: 82.2min\n",
      "\n",
      "building tree 46 of 150[Parallel(n_jobs=2)]: Done  44 tasks      | elapsed: 83.2min\n",
      "\n",
      "building tree 47 of 150[Parallel(n_jobs=2)]: Done  45 tasks      | elapsed: 85.9min\n",
      "\n",
      "building tree 48 of 150[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 86.9min\n",
      "\n",
      "building tree 49 of 150[Parallel(n_jobs=2)]: Done  47 tasks      | elapsed: 89.4min\n",
      "\n",
      "building tree 50 of 150[Parallel(n_jobs=2)]: Done  48 tasks      | elapsed: 90.6min\n",
      "\n",
      "building tree 51 of 150[Parallel(n_jobs=2)]: Done  49 tasks      | elapsed: 93.1min\n",
      "\n",
      "building tree 52 of 150[Parallel(n_jobs=2)]: Done  50 tasks      | elapsed: 94.3min\n",
      "\n",
      "building tree 53 of 150[Parallel(n_jobs=2)]: Done  51 tasks      | elapsed: 97.0min\n",
      "\n",
      "building tree 54 of 150[Parallel(n_jobs=2)]: Done  52 tasks      | elapsed: 98.1min\n",
      "\n",
      "building tree 55 of 150[Parallel(n_jobs=2)]: Done  53 tasks      | elapsed: 100.8min\n",
      "\n",
      "building tree 56 of 150[Parallel(n_jobs=2)]: Done  54 tasks      | elapsed: 101.7min\n",
      "\n",
      "building tree 57 of 150[Parallel(n_jobs=2)]: Done  55 tasks      | elapsed: 104.3min\n",
      "\n",
      "building tree 58 of 150[Parallel(n_jobs=2)]: Done  56 tasks      | elapsed: 105.5min\n",
      "\n",
      "building tree 59 of 150[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed: 108.1min\n",
      "\n",
      "building tree 60 of 150[Parallel(n_jobs=2)]: Done  58 tasks      | elapsed: 109.2min\n",
      "\n",
      "building tree 61 of 150[Parallel(n_jobs=2)]: Done  59 tasks      | elapsed: 111.7min\n",
      "\n",
      "building tree 62 of 150[Parallel(n_jobs=2)]: Done  60 tasks      | elapsed: 112.8min\n",
      "\n",
      "building tree 63 of 150[Parallel(n_jobs=2)]: Done  61 tasks      | elapsed: 115.4min\n",
      "\n",
      "building tree 64 of 150[Parallel(n_jobs=2)]: Done  62 tasks      | elapsed: 116.5min\n",
      "\n",
      "building tree 65 of 150[Parallel(n_jobs=2)]: Done  63 tasks      | elapsed: 119.3min\n",
      "\n",
      "building tree 66 of 150[Parallel(n_jobs=2)]: Done  64 tasks      | elapsed: 120.2min\n",
      "\n",
      "building tree 67 of 150[Parallel(n_jobs=2)]: Done  65 tasks      | elapsed: 123.0min\n",
      "\n",
      "building tree 68 of 150[Parallel(n_jobs=2)]: Done  66 tasks      | elapsed: 124.1min\n",
      "\n",
      "building tree 69 of 150[Parallel(n_jobs=2)]: Done  67 tasks      | elapsed: 126.8min\n",
      "\n",
      "building tree 70 of 150[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed: 127.7min\n",
      "\n",
      "building tree 71 of 150[Parallel(n_jobs=2)]: Done  69 tasks      | elapsed: 130.5min\n",
      "\n",
      "building tree 72 of 150[Parallel(n_jobs=2)]: Done  70 tasks      | elapsed: 131.3min\n",
      "\n",
      "building tree 73 of 150[Parallel(n_jobs=2)]: Done  71 tasks      | elapsed: 134.3min\n",
      "\n",
      "building tree 74 of 150[Parallel(n_jobs=2)]: Done  72 tasks      | elapsed: 134.9min\n",
      "\n",
      "building tree 75 of 150[Parallel(n_jobs=2)]: Done  73 tasks      | elapsed: 138.3min\n",
      "\n",
      "building tree 76 of 150[Parallel(n_jobs=2)]: Done  74 tasks      | elapsed: 138.6min\n",
      "\n",
      "building tree 77 of 150[Parallel(n_jobs=2)]: Done  75 tasks      | elapsed: 142.0min\n",
      "\n",
      "building tree 78 of 150[Parallel(n_jobs=2)]: Done  76 tasks      | elapsed: 142.3min\n",
      "\n",
      "building tree 79 of 150[Parallel(n_jobs=2)]: Done  77 tasks      | elapsed: 146.0min\n",
      "\n",
      "building tree 80 of 150[Parallel(n_jobs=2)]: Done  78 tasks      | elapsed: 146.1min\n",
      "\n",
      "building tree 81 of 150[Parallel(n_jobs=2)]: Done  79 tasks      | elapsed: 149.8min\n",
      "\n",
      "building tree 82 of 150[Parallel(n_jobs=2)]: Done  80 tasks      | elapsed: 149.9min\n",
      "\n",
      "building tree 83 of 150[Parallel(n_jobs=2)]: Done  81 tasks      | elapsed: 153.5min\n",
      "\n",
      "building tree 84 of 150[Parallel(n_jobs=2)]: Done  82 tasks      | elapsed: 153.6min\n",
      "\n",
      "building tree 85 of 150[Parallel(n_jobs=2)]: Done  83 tasks      | elapsed: 157.4min\n",
      "\n",
      "building tree 86 of 150[Parallel(n_jobs=2)]: Done  84 tasks      | elapsed: 157.5min\n",
      "\n",
      "building tree 87 of 150[Parallel(n_jobs=2)]: Done  85 tasks      | elapsed: 161.2min\n",
      "\n",
      "building tree 88 of 150[Parallel(n_jobs=2)]: Done  86 tasks      | elapsed: 161.3min\n",
      "\n",
      "building tree 89 of 150[Parallel(n_jobs=2)]: Done  87 tasks      | elapsed: 165.0min\n",
      "\n",
      "building tree 90 of 150[Parallel(n_jobs=2)]: Done  88 tasks      | elapsed: 165.0min\n",
      "\n",
      "building tree 91 of 150[Parallel(n_jobs=2)]: Done  89 tasks      | elapsed: 168.7min\n",
      "\n",
      "building tree 92 of 150[Parallel(n_jobs=2)]: Done  90 tasks      | elapsed: 168.9min\n",
      "\n",
      "building tree 93 of 150[Parallel(n_jobs=2)]: Done  91 tasks      | elapsed: 172.4min\n",
      "\n",
      "building tree 94 of 150[Parallel(n_jobs=2)]: Done  92 tasks      | elapsed: 172.6min\n",
      "\n",
      "building tree 95 of 150[Parallel(n_jobs=2)]: Done  93 tasks      | elapsed: 176.2min\n",
      "\n",
      "building tree 96 of 150[Parallel(n_jobs=2)]: Done  94 tasks      | elapsed: 176.3min\n",
      "\n",
      "building tree 97 of 150[Parallel(n_jobs=2)]: Done  95 tasks      | elapsed: 179.9min\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 98 of 150[Parallel(n_jobs=2)]: Done  96 tasks      | elapsed: 180.1min\n",
      "\n",
      "building tree 99 of 150[Parallel(n_jobs=2)]: Done  97 tasks      | elapsed: 183.6min\n",
      "\n",
      "building tree 100 of 150[Parallel(n_jobs=2)]: Done  98 tasks      | elapsed: 184.0min\n",
      "\n",
      "building tree 101 of 150[Parallel(n_jobs=2)]: Done  99 tasks      | elapsed: 187.4min\n",
      "\n",
      "building tree 102 of 150[Parallel(n_jobs=2)]: Done 100 tasks      | elapsed: 187.7min\n",
      "\n",
      "building tree 103 of 150[Parallel(n_jobs=2)]: Done 101 tasks      | elapsed: 191.1min\n",
      "\n",
      "building tree 104 of 150[Parallel(n_jobs=2)]: Done 102 tasks      | elapsed: 191.4min\n",
      "\n",
      "building tree 105 of 150[Parallel(n_jobs=2)]: Done 103 tasks      | elapsed: 194.5min\n",
      "\n",
      "building tree 106 of 150[Parallel(n_jobs=2)]: Done 104 tasks      | elapsed: 195.2min\n",
      "\n",
      "building tree 107 of 150[Parallel(n_jobs=2)]: Done 105 tasks      | elapsed: 198.2min\n",
      "\n",
      "building tree 108 of 150[Parallel(n_jobs=2)]: Done 106 tasks      | elapsed: 199.1min\n",
      "\n",
      "building tree 109 of 150[Parallel(n_jobs=2)]: Done 107 tasks      | elapsed: 202.1min\n",
      "\n",
      "building tree 110 of 150[Parallel(n_jobs=2)]: Done 108 tasks      | elapsed: 202.8min\n",
      "\n",
      "building tree 111 of 150[Parallel(n_jobs=2)]: Done 109 tasks      | elapsed: 206.0min\n",
      "\n",
      "building tree 112 of 150[Parallel(n_jobs=2)]: Done 110 tasks      | elapsed: 206.4min\n",
      "\n",
      "building tree 113 of 150[Parallel(n_jobs=2)]: Done 111 tasks      | elapsed: 209.7min\n",
      "\n",
      "building tree 114 of 150[Parallel(n_jobs=2)]: Done 112 tasks      | elapsed: 210.0min\n",
      "\n",
      "building tree 115 of 150[Parallel(n_jobs=2)]: Done 113 tasks      | elapsed: 213.3min\n",
      "\n",
      "building tree 116 of 150[Parallel(n_jobs=2)]: Done 114 tasks      | elapsed: 213.6min\n",
      "\n",
      "building tree 117 of 150[Parallel(n_jobs=2)]: Done 115 tasks      | elapsed: 217.2min\n",
      "\n",
      "building tree 118 of 150[Parallel(n_jobs=2)]: Done 116 tasks      | elapsed: 217.4min\n",
      "\n",
      "building tree 119 of 150[Parallel(n_jobs=2)]: Done 117 tasks      | elapsed: 221.0min\n",
      "\n",
      "building tree 120 of 150[Parallel(n_jobs=2)]: Done 118 tasks      | elapsed: 221.1min\n",
      "\n",
      "building tree 121 of 150[Parallel(n_jobs=2)]: Done 119 tasks      | elapsed: 224.7min\n",
      "\n",
      "building tree 122 of 150[Parallel(n_jobs=2)]: Done 120 tasks      | elapsed: 224.9min\n",
      "\n",
      "building tree 123 of 150[Parallel(n_jobs=2)]: Done 121 tasks      | elapsed: 228.7min\n",
      "\n",
      "building tree 124 of 150[Parallel(n_jobs=2)]: Done 122 tasks      | elapsed: 228.7min\n",
      "\n",
      "building tree 125 of 150[Parallel(n_jobs=2)]: Done 123 tasks      | elapsed: 232.3min\n",
      "\n",
      "building tree 126 of 150[Parallel(n_jobs=2)]: Done 124 tasks      | elapsed: 232.4min\n",
      "\n",
      "building tree 127 of 150[Parallel(n_jobs=2)]: Done 125 tasks      | elapsed: 236.0min\n",
      "\n",
      "building tree 128 of 150[Parallel(n_jobs=2)]: Done 126 tasks      | elapsed: 236.2min\n",
      "\n",
      "building tree 129 of 150[Parallel(n_jobs=2)]: Done 127 tasks      | elapsed: 239.7min\n",
      "\n",
      "building tree 130 of 150[Parallel(n_jobs=2)]: Done 128 tasks      | elapsed: 240.0min\n",
      "\n",
      "building tree 131 of 150[Parallel(n_jobs=2)]: Done 129 tasks      | elapsed: 243.2min\n",
      "\n",
      "building tree 132 of 150[Parallel(n_jobs=2)]: Done 130 tasks      | elapsed: 243.5min\n",
      "\n",
      "building tree 133 of 150[Parallel(n_jobs=2)]: Done 131 tasks      | elapsed: 246.9min\n",
      "\n",
      "building tree 134 of 150[Parallel(n_jobs=2)]: Done 132 tasks      | elapsed: 247.2min\n",
      "\n",
      "building tree 135 of 150[Parallel(n_jobs=2)]: Done 133 tasks      | elapsed: 250.5min\n",
      "\n",
      "building tree 136 of 150[Parallel(n_jobs=2)]: Done 134 tasks      | elapsed: 250.9min\n",
      "\n",
      "building tree 137 of 150[Parallel(n_jobs=2)]: Done 135 tasks      | elapsed: 254.3min\n",
      "\n",
      "building tree 138 of 150[Parallel(n_jobs=2)]: Done 136 tasks      | elapsed: 254.4min\n",
      "\n",
      "building tree 139 of 150[Parallel(n_jobs=2)]: Done 137 tasks      | elapsed: 257.9min\n",
      "\n",
      "building tree 140 of 150[Parallel(n_jobs=2)]: Done 138 tasks      | elapsed: 258.1min\n",
      "\n",
      "building tree 141 of 150[Parallel(n_jobs=2)]: Done 139 tasks      | elapsed: 261.5min\n",
      "\n",
      "building tree 142 of 150[Parallel(n_jobs=2)]: Done 140 tasks      | elapsed: 261.9min\n",
      "\n",
      "building tree 143 of 150[Parallel(n_jobs=2)]: Done 141 tasks      | elapsed: 265.1min\n",
      "\n",
      "building tree 144 of 150[Parallel(n_jobs=2)]: Done 142 tasks      | elapsed: 265.5min\n",
      "\n",
      "building tree 145 of 150[Parallel(n_jobs=2)]: Done 143 tasks      | elapsed: 268.7min\n",
      "\n",
      "building tree 146 of 150[Parallel(n_jobs=2)]: Done 144 tasks      | elapsed: 269.4min\n",
      "\n",
      "building tree 147 of 150[Parallel(n_jobs=2)]: Done 145 tasks      | elapsed: 272.4min\n",
      "\n",
      "building tree 148 of 150[Parallel(n_jobs=2)]: Done 146 tasks      | elapsed: 273.5min\n",
      "\n",
      "building tree 149 of 150[Parallel(n_jobs=2)]: Done 147 tasks      | elapsed: 276.1min\n",
      "\n",
      "building tree 150 of 150\n",
      "[Parallel(n_jobs=2)]: Done 150 out of 150 | elapsed: 280.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=2,\n",
       "                       oob_score=False, random_state=None, verbose=100,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate an ensemble random forest classifier with no of estimator with 2 jobs and train\n",
    "clf = RandomForestClassifier(n_estimators=150, verbose=100, n_jobs=2)\n",
    "clf.fit(training_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   2 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   3 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=2)]: Done   5 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=2)]: Done   6 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=2)]: Done   7 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=2)]: Done   8 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=2)]: Done  10 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=2)]: Done  11 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=2)]: Done  12 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=2)]: Done  13 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=2)]: Done  15 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=2)]: Done  16 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=2)]: Done  17 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=2)]: Done  18 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=2)]: Done  19 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=2)]: Done  20 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=2)]: Done  22 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=2)]: Done  23 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=2)]: Done  24 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=2)]: Done  25 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=2)]: Done  26 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=2)]: Done  27 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=2)]: Done  29 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=2)]: Done  30 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=2)]: Done  31 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=2)]: Done  32 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=2)]: Done  33 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=2)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=2)]: Done  35 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=2)]: Done  36 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=2)]: Done  38 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=2)]: Done  39 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=2)]: Done  40 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=2)]: Done  41 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=2)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=2)]: Done  43 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=2)]: Done  44 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=2)]: Done  45 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=2)]: Done  47 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=2)]: Done  48 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=2)]: Done  49 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=2)]: Done  50 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=2)]: Done  51 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=2)]: Done  52 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=2)]: Done  53 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=2)]: Done  54 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=2)]: Done  55 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=2)]: Done  56 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=2)]: Done  58 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=2)]: Done  59 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=2)]: Done  60 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=2)]: Done  61 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=2)]: Done  62 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=2)]: Done  63 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=2)]: Done  64 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=2)]: Done  65 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=2)]: Done  66 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=2)]: Done  67 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=2)]: Done  69 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=2)]: Done  70 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=2)]: Done  71 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=2)]: Done  72 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=2)]: Done  73 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=2)]: Done  74 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=2)]: Done  75 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=2)]: Done  76 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=2)]: Done  77 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=2)]: Done  78 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=2)]: Done  79 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=2)]: Done  80 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=2)]: Done  81 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=2)]: Done  82 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=2)]: Done  83 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=2)]: Done  84 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=2)]: Done  85 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=2)]: Done  86 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=2)]: Done  87 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=2)]: Done  88 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=2)]: Done  89 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=2)]: Done  90 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=2)]: Done  91 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=2)]: Done  92 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=2)]: Done  93 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=2)]: Done  94 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=2)]: Done  95 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=2)]: Done  96 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=2)]: Done  97 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=2)]: Done  98 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=2)]: Done  99 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=2)]: Done 100 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=2)]: Done 101 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=2)]: Done 102 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=2)]: Done 103 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=2)]: Done 104 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=2)]: Done 105 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=2)]: Done 106 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=2)]: Done 107 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=2)]: Done 108 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=2)]: Done 109 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=2)]: Done 110 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=2)]: Done 111 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=2)]: Done 112 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=2)]: Done 113 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=2)]: Done 114 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=2)]: Done 115 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=2)]: Done 116 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=2)]: Done 117 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=2)]: Done 118 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=2)]: Done 119 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=2)]: Done 120 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=2)]: Done 121 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=2)]: Done 122 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=2)]: Done 123 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=2)]: Done 124 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=2)]: Done 125 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=2)]: Done 126 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=2)]: Done 127 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=2)]: Done 128 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=2)]: Done 129 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=2)]: Done 130 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=2)]: Done 131 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=2)]: Done 132 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=2)]: Done 133 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=2)]: Done 134 tasks      | elapsed:   19.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 135 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=2)]: Done 136 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=2)]: Done 137 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=2)]: Done 138 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=2)]: Done 139 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=2)]: Done 140 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=2)]: Done 141 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=2)]: Done 142 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=2)]: Done 143 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=2)]: Done 144 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=2)]: Done 145 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=2)]: Done 146 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=2)]: Done 147 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=2)]: Done 150 out of 150 | elapsed:   22.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Predict given the testing features\n",
    "y_pred = clf.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8641875\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how much is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes accuracy could be a bit misleading and hence we calculate the confusion matrix which gives us a better sense as to how many were classified correctly and how many were not.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[134611,  17304],\n",
       "       [ 23983, 128102]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134611</td>\n",
       "      <td>17304</td>\n",
       "      <td>151915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23983</td>\n",
       "      <td>128102</td>\n",
       "      <td>152085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>158594</td>\n",
       "      <td>145406</td>\n",
       "      <td>304000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted       0       1     All\n",
       "True                             \n",
       "0          134611   17304  151915\n",
       "1           23983  128102  152085\n",
       "All        158594  145406  304000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the confusion matrix into pandas dataframe\n",
    "y_true = pd.Series(y_test)\n",
    "y_predicted = pd.Series(y_pred)\n",
    "\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this section, we calculate the importance of the features. We are **not particularly interested** in the `neghboring_words` since they are just bag of words and we have used hashing so it is hard to trace. But, we could check the importance of the `neighboring_tags` since these were the first 35 columns in the stacked feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the importances and sort them according to decreasing order\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 11), (9, 13), (10, 1), (11, 26), (15, 10), (19, 6), (24, 5), (28, 0), (36, 25), (38, 2), (44, 17), (47, 30), (58, 28), (60, 18), (62, 29), (78, 23), (89, 32), (115, 27), (122, 16), (139, 4), (184, 12), (277, 9), (406, 31), (512, 7), (524, 8), (525, 19), (526, 33), (527, 34), (528, 21), (529, 20), (530, 3), (531, 15), (532, 14), (533, 22), (534, 24)]\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the only the neighboring_tags feature which we are interested in\n",
    "interested_indices = list(range(35))\n",
    "\n",
    "get_ranking_tags = [(i,j) for i, j in enumerate(indices) if j in interested_indices]\n",
    "print(get_ranking_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that for `neighboring_tags` the highest importance is hold by tags such as `NNP (Proper nouns)`, `NNS (Plurals)`, `CD (Cardinal Number)` and `VBD (Past tense)` which is correct with the hypothesis since most of the times when there are scientific citation something is either being claimed for a particular time or for a particular person, so these tags are really important and show how POS tags can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnp\n",
      "nns\n",
      "cd\n",
      "vbd\n",
      "nn\n",
      "jj\n",
      "in\n",
      "cc\n",
      "vb\n",
      "dt\n",
      "prp\n",
      "vbz\n",
      "vbn\n",
      "rb\n",
      "vbp\n",
      "to\n",
      "wikicode\n",
      "vbg\n",
      "pos\n",
      "fw\n",
      "nnps\n",
      "md\n",
      "wdt\n",
      "jjr\n",
      "jjs\n",
      "rbr\n",
      "wp\n",
      "wrb\n",
      "rp\n",
      "rbs\n",
      "ex\n",
      "pdt\n",
      "others\n",
      "sym\n",
      "uh\n"
     ]
    }
   ],
   "source": [
    "for i, j in get_ranking_tags:\n",
    "    print(cv.get_feature_names()[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
