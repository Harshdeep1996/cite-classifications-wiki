{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import glob\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initializing tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x.name for x in local_device_protos if x.device_type == 'GPU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_features = pd.read_parquet('/dlabdata1/harshdee/citations_features.parquet/', engine='pyarrow')\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the citation and their corresponding features which have been extracted\n",
    "book_journal_features = pd.merge(\n",
    "    dataset, citations_features, how='inner', left_on=['id','citation'], right_on = ['id','citation']\n",
    ")\n",
    "book_journal_features.drop('page_title_y', axis=1, inplace=True)\n",
    "book_journal_features.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1754055, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_journal_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider unique citations so that the dataset is more varied\n",
    "book_journal_features = book_journal_features.set_index(['id', 'citation'])\n",
    "book_journal_features = book_journal_features[~book_journal_features.index.duplicated(keep='first')]\n",
    "book_journal_features = book_journal_features.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get auxiliary features and divide them into labels\n",
    "\n",
    "1. `ref_index`\n",
    "2. `total_words`\n",
    "3. `tags`\n",
    "4. `type_of_citation`\n",
    "\n",
    "#### can include `section` of the page in which the citation belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_journal_features['actual_label'] = 'rest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_journal_features.loc[~pd.isna(book_journal_features['PMC']), ['actual_label']] = 'journal'\n",
    "book_journal_features.loc[~pd.isna(book_journal_features['PMID']), ['actual_label']] = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_doi = (\n",
    "    ~pd.isna(book_journal_features['DOI']) & \n",
    "    pd.isna(book_journal_features['PMC']) & \n",
    "    pd.isna(book_journal_features['PMID']) &\n",
    "    pd.isna(book_journal_features['ISBN'])\n",
    ")\n",
    "book_journal_features.loc[only_doi, ['actual_label']] = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_book = (\n",
    "    ~pd.isna(book_journal_features['ISBN']) & \n",
    "    pd.isna(book_journal_features['PMC']) & \n",
    "    pd.isna(book_journal_features['PMID']) &\n",
    "    pd.isna(book_journal_features['DOI'])\n",
    ")\n",
    "book_journal_features.loc[only_book, ['actual_label']] = 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_book_and_doi_journal = (\n",
    "    ~pd.isna(book_journal_features['ISBN']) & \n",
    "    ~pd.isna(book_journal_features['DOI']) & \n",
    "    pd.isna(book_journal_features['PMID']) &\n",
    "    pd.isna(book_journal_features['PMC']) &\n",
    "    book_journal_features['citation_type'].isin(['cite journal', 'cite conference'])\n",
    ")\n",
    "book_journal_features.loc[both_book_and_doi_journal, ['actual_label']] = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_book_and_doi_book = (\n",
    "    ~pd.isna(book_journal_features['ISBN']) & \n",
    "    ~pd.isna(book_journal_features['DOI']) & \n",
    "    pd.isna(book_journal_features['PMID']) &\n",
    "    pd.isna(book_journal_features['PMC']) &\n",
    "    book_journal_features['citation_type'].isin(['cite book', 'cite encyclopedia'])\n",
    ")\n",
    "book_journal_features.loc[both_book_and_doi_book, ['actual_label']] = 'book'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402511, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Made the dataset which contains citations book and journal labeled\n",
    "book_journal_features = book_journal_features[book_journal_features['actual_label'].isin(['book', 'journal'])]\n",
    "book_journal_features = book_journal_features[[\n",
    "    'sections', 'citation_type', 'citation', 'id', 'ref_index',\n",
    "     'total_words', 'neighboring_tags', 'actual_label', 'neighboring_words'\n",
    "]]\n",
    "book_journal_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite journal | author= Kenneth Cornetta | author2 = W.French Anderson | title = Protamine sulfate as an effective alternative to polybrene in retroviral-mediated gene-transfer: implications for human gene therapy | journal = Journal of Virological Methods | year= 1989 | volume= 23 | issue= 2 | pages= 187\\\\u2013194 | url=http://www.sciencedirect.com/science/article/pii/0166093489901328 | doi=10.1016/0166-0934(89)90132-8 | pmid= 2786000}}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_journal_features.iloc[0]['citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1402511/1402511 [00:03<00:00, 391859.57it/s]\n",
      "100%|██████████| 1402511/1402511 [00:03<00:00, 383214.35it/s]\n",
      "100%|██████████| 1402511/1402511 [00:04<00:00, 326432.12it/s]\n",
      "100%|██████████| 1402511/1402511 [00:03<00:00, 355127.63it/s]\n"
     ]
    }
   ],
   "source": [
    "book_journal_features['citation'] = book_journal_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('url\\s{0,10}=\\s{0,10}([^|]+)', 'url = ', x))\n",
    "book_journal_features['citation'] = book_journal_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('work\\s{0,10}=\\s{0,10}([^|]+)', 'work = ', x))\n",
    "book_journal_features['citation'] = book_journal_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('newspaper\\s{0,10}=\\s{0,10}([^|]+)', 'newspaper = ', x))\n",
    "book_journal_features['citation'] = book_journal_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('website\\s{0,10}=\\s{0,10}([^|]+)', 'website = ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite journal | author= Kenneth Cornetta | author2 = W.French Anderson | title = Protamine sulfate as an effective alternative to polybrene in retroviral-mediated gene-transfer: implications for human gene therapy | journal = Journal of Virological Methods | year= 1989 | volume= 23 | issue= 2 | pages= 187\\\\u2013194 | url = | doi=10.1016/0166-0934(89)90132-8 | pmid= 2786000}}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_journal_features.iloc[0]['citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388908, 35)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading the dataset of newspapers which was generated from the citations_separated dataset\n",
    "li = []\n",
    "all_files = glob.glob('/dlabdata1/harshdee/newspapers_citations_features.csv/' + \"/*.csv\")\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, header=None, sep='\\t')\n",
    "    li.append(df)\n",
    "\n",
    "newspaper_data = pd.concat(li, axis=0, ignore_index=True)\n",
    "newspaper_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaper_data = newspaper_data[[0, 1, 2, 3, 4, 28, 32, 33]]\n",
    "newspaper_data.rename({\n",
    "    0: 'citation', 1: 'ref_index', 2: 'total_words',\n",
    "    3: 'neighboring_words', 4: 'neighboring_tags', \n",
    "    28: 'id', 32: 'sections', 33: 'citation_type'}, axis=1, inplace=True)\n",
    "newspaper_data['actual_label'] = 'newspaper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite news | accessdate=January 30, 2016|work= New York Times| first = Henry | last = Raymont | url = https://timesmachine.nytimes.com/timesmachine/1971/11/14/79406266.pdf | title = U.S. Shift on Cuba in 1960 Detailed | date = November 14, 1971}}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaper_data.iloc[819218]['citation'] ## Example before removing the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1388908/1388908 [00:04<00:00, 280428.82it/s]\n",
      "100%|██████████| 1388908/1388908 [00:03<00:00, 366677.25it/s]\n",
      "100%|██████████| 1388908/1388908 [00:03<00:00, 424954.28it/s]\n",
      "100%|██████████| 1388908/1388908 [00:03<00:00, 447678.22it/s]\n"
     ]
    }
   ],
   "source": [
    "newspaper_data['citation'] = newspaper_data['citation'].progress_apply(\n",
    "    lambda x: re.sub('url\\s{0,10}=\\s{0,10}([^|]+)', 'url = ', x))\n",
    "newspaper_data['citation'] = newspaper_data['citation'].progress_apply(\n",
    "    lambda x: re.sub('work\\s{0,10}=\\s{0,10}([^|]+)', 'work = ', x))\n",
    "newspaper_data['citation'] = newspaper_data['citation'].progress_apply(\n",
    "    lambda x: re.sub('newspaper\\s{0,10}=\\s{0,10}([^|]+)', 'newspaper = ', x))\n",
    "newspaper_data['citation'] = newspaper_data['citation'].progress_apply(\n",
    "    lambda x: re.sub('website\\s{0,10}=\\s{0,10}([^|]+)', 'website = ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite news | accessdate=January 30, 2016|work = | first = Henry | last = Raymont | url = | title = U.S. Shift on Cuba in 1960 Detailed | date = November 14, 1971}}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaper_data.iloc[819218]['citation'] ## Example after removing the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment_features = pd.read_parquet(\n",
    "    '/dlabdata1/harshdee/entertainment_citations_complete.parquet/', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment_features = entertainment_features[[\n",
    "    'ref_index', 'total_words', 'neighboring_words', 'neighboring_tags', 'id', 'sections', 'citations']]\n",
    "entertainment_features.rename({'citations': 'citation'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite web|url=http://www.billboard.com/charts/2010-03-06/latin-albums|title=Latin Albums: Week of March 06, 2010|date=March 6, 2010|work=Billboard|publisher=Prometheus Global Media|accessdate=March 15, 2012}}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment_features.iloc[23787]['citation'] ## Example before removing the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment_features['actual_label'] = 'entertainment'\n",
    "newspaper_data.drop('citation_type', axis=1, inplace=True)\n",
    "book_journal_features.drop('citation_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 609218/609218 [00:01<00:00, 354227.78it/s]\n",
      "100%|██████████| 609218/609218 [00:01<00:00, 354009.94it/s]\n",
      "100%|██████████| 609218/609218 [00:01<00:00, 354752.91it/s]\n",
      "100%|██████████| 609218/609218 [00:01<00:00, 384490.74it/s]\n"
     ]
    }
   ],
   "source": [
    "entertainment_features['citation'] = entertainment_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('url\\s{0,10}=\\s{0,10}([^|]+)', 'url = ', x))\n",
    "entertainment_features['citation'] = entertainment_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('work\\s{0,10}=\\s{0,10}([^|]+)', 'work = ', x))\n",
    "entertainment_features['citation'] = entertainment_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('newspaper\\s{0,10}=\\s{0,10}([^|]+)', 'newspaper = ', x))\n",
    "entertainment_features['citation'] = entertainment_features['citation'].progress_apply(\n",
    "    lambda x: re.sub('website\\s{0,10}=\\s{0,10}([^|]+)', 'website = ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{cite web|url = |title=Latin Albums: Week of March 06, 2010|date=March 6, 2010|work = |publisher=Prometheus Global Media|accessdate=March 15, 2012}}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment_features.iloc[23787]['citation'] ## Example after removing the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 609218/609218 [00:01<00:00, 409267.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cite web               489586\n",
       "cite news               39991\n",
       "Cite web                33099\n",
       "Citation                18787\n",
       "Cite news               13013\n",
       "cite journal             5078\n",
       "cite AV media            4489\n",
       "citation                 1074\n",
       "cite episode              703\n",
       "Cite AV media notes       577\n",
       "Cite AV media             532\n",
       "cite press release        455\n",
       "cite interview            451\n",
       "Cite episode              410\n",
       "cite AV media notes       324\n",
       "cite book                 266\n",
       "Cite journal              175\n",
       "cite podcast               51\n",
       "cite speech                49\n",
       "cite conference            28\n",
       "Cite book                  20\n",
       "Cite press release         19\n",
       "Cite interview             16\n",
       "cite serial                 8\n",
       "Cite speech                 4\n",
       "Cite conference             3\n",
       "cite report                 2\n",
       "cite encyclopedia           2\n",
       "cite DVD notes              2\n",
       "Cite report                 2\n",
       "Cite encyclopedia           1\n",
       "Cite podcast                1\n",
       "Name: citation, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment_features['citation'].progress_apply(lambda x: re.findall('{{\\s{0,10}([^|]+)', x)[0].strip()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3400637, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features = pd.concat([book_journal_features, newspaper_data, entertainment_features])\n",
    "dataset_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataset_with_features['actual_label'])\n",
    "dataset_with_features['label_category'] = le.transform(dataset_with_features['actual_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>sections</th>\n",
       "      <th>total_words</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>{{Citation | title = YouTube | url = | contrib...</td>\n",
       "      <td>15511</td>\n",
       "      <td>[JJ, NNP, VBD, RB, JJ, NN, :, NN, :, JJ, NN, :...</td>\n",
       "      <td>[last, Horsley, deadurl, yes, archiveurl, http...</td>\n",
       "      <td>1625</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>8158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual_label                                           citation     id  \\\n",
       "0  entertainment  {{Citation | title = YouTube | url = | contrib...  15511   \n",
       "\n",
       "                                    neighboring_tags  \\\n",
       "0  [JJ, NNP, VBD, RB, JJ, NN, :, NN, :, JJ, NN, :...   \n",
       "\n",
       "                                   neighboring_words  ref_index  \\\n",
       "0  [last, Horsley, deadurl, yes, archiveurl, http...       1625   \n",
       "\n",
       "          sections  total_words  label_category  \n",
       "0  Initial Section         8158               1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['actual_label'] == 'entertainment'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>sections</th>\n",
       "      <th>total_words</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>{{Citation | date = 10 December 2012 | url = |...</td>\n",
       "      <td>469553</td>\n",
       "      <td>[CD,NN,NN,CD,NN,NN,NN,'',NNP,NNP,'',NN,NN,CD,N...</td>\n",
       "      <td>[99,issue,page,463,ref,ref,name=,'',Lovelace,G...</td>\n",
       "      <td>1427</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>2322</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual_label                                           citation      id  \\\n",
       "0    newspaper  {{Citation | date = 10 December 2012 | url = |...  469553   \n",
       "\n",
       "                                    neighboring_tags  \\\n",
       "0  [CD,NN,NN,CD,NN,NN,NN,'',NNP,NNP,'',NN,NN,CD,N...   \n",
       "\n",
       "                                   neighboring_words  ref_index  \\\n",
       "0  [99,issue,page,463,ref,ref,name=,'',Lovelace,G...       1427   \n",
       "\n",
       "          sections  total_words  label_category  \n",
       "0  Initial Section         2322               3  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['actual_label'] == 'newspaper'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>sections</th>\n",
       "      <th>total_words</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>book</td>\n",
       "      <td>{{cite book|author1=David John Cole|author2=Ev...</td>\n",
       "      <td>1831574</td>\n",
       "      <td>[NN, NNP, NNP, NNP, NNP, NNP, NNP, IN, NNP, NN...</td>\n",
       "      <td>[bookauthor1David, John, Coleauthor2Eve, Brown...</td>\n",
       "      <td>110</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>1242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label                                           citation       id  \\\n",
       "34         book  {{cite book|author1=David John Cole|author2=Ev...  1831574   \n",
       "\n",
       "                                     neighboring_tags  \\\n",
       "34  [NN, NNP, NNP, NNP, NNP, NNP, NNP, IN, NNP, NN...   \n",
       "\n",
       "                                    neighboring_words  ref_index  \\\n",
       "34  [bookauthor1David, John, Coleauthor2Eve, Brown...        110   \n",
       "\n",
       "           sections  total_words  label_category  \n",
       "34  Initial Section         1242               0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['actual_label'] == 'book'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>neighboring_words</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>sections</th>\n",
       "      <th>total_words</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journal</td>\n",
       "      <td>{{cite journal | author= Kenneth Cornetta | au...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>[NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...</td>\n",
       "      <td>[Methods, year, 1989, volume, 23, issue, pages...</td>\n",
       "      <td>941</td>\n",
       "      <td>Initial Section</td>\n",
       "      <td>1661</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual_label                                           citation       id  \\\n",
       "0      journal  {{cite journal | author= Kenneth Cornetta | au...  1831220   \n",
       "\n",
       "                                    neighboring_tags  \\\n",
       "0  [NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...   \n",
       "\n",
       "                                   neighboring_words  ref_index  \\\n",
       "0  [Methods, year, 1989, volume, 23, issue, pages...        941   \n",
       "\n",
       "          sections  total_words  label_category  \n",
       "0  Initial Section         1661               2  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features[dataset_with_features['actual_label'] == 'journal'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## clearing up memory\n",
    "del citations_features\n",
    "del dataset\n",
    "del book_journal_features\n",
    "del newspaper_data\n",
    "del entertainment_features\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rows which have duplicate ID and citations since they are just the same examples\n",
    "dataset_with_features = dataset_with_features.drop_duplicates(subset=['id', 'citation']) ## keeps first row\n",
    "dataset_with_features = dataset_with_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3053972, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please save this file and use it - as an intermediate file if you want to use it somewhere else\n",
    "## dataset_with_features.to_csv('dataset_with_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the unique `sections` and one hot encoding it to get a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only processing auxiliary features which are going to be used in the neural network\n",
    "auxiliary_features = dataset_with_features[\n",
    "    ['sections', 'citation', 'id', 'ref_index',\n",
    "     'total_words', 'neighboring_tags', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features['sections'] = auxiliary_features['sections'].astype(str)\n",
    "auxiliary_features['sections'] = auxiliary_features['sections'].apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_counts = pd.Series(Counter(chain.from_iterable(x for x in auxiliary_features.sections)))\n",
    "largest_sections = section_counts.nlargest(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [00:09<00:00, 306834.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Change section to `OTHERS` if occurence of the section is not in the 150 largest sections\n",
    "auxiliary_features['sections'] = auxiliary_features['sections'].progress_apply(\n",
    "    lambda x: list(set(['Others' if i not in largest_sections else i for i in x]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sections</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>label_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Initial Section]</td>\n",
       "      <td>{{cite journal | author= Kenneth Cornetta | au...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>941</td>\n",
       "      <td>1661</td>\n",
       "      <td>[NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Initial Section]</td>\n",
       "      <td>{{cite journal|last=Sorgi|first=FL|author2=Bha...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1025</td>\n",
       "      <td>1661</td>\n",
       "      <td>[NN, ,, NNP, ,, NNP, NN, NNS, JJ, NN, NN, VBD,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Initial Section]</td>\n",
       "      <td>{{cite journal|last=Walker|first=WS|author2=Re...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1187</td>\n",
       "      <td>1661</td>\n",
       "      <td>[,, NNP, ., JJ, JJ, NN, IN, NNS, IN, JJ, NNS, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Initial Section]</td>\n",
       "      <td>{{cite journal|last=Campbell|first=FW|author2=...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1267</td>\n",
       "      <td>1661</td>\n",
       "      <td>[,, NNP, ,, NN, ., NN, IN, DT, NN, IN, JJ, NN,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Initial Section]</td>\n",
       "      <td>{{cite journal|last=Welsby|first=IJ|author2=Ne...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1364</td>\n",
       "      <td>1661</td>\n",
       "      <td>[JJ, NNS, IN, JJ, NN, NN, IN, NN, IN, JJ, NN, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sections                                           citation  \\\n",
       "0  [Initial Section]  {{cite journal | author= Kenneth Cornetta | au...   \n",
       "1  [Initial Section]  {{cite journal|last=Sorgi|first=FL|author2=Bha...   \n",
       "2  [Initial Section]  {{cite journal|last=Walker|first=WS|author2=Re...   \n",
       "3  [Initial Section]  {{cite journal|last=Campbell|first=FW|author2=...   \n",
       "4  [Initial Section]  {{cite journal|last=Welsby|first=IJ|author2=Ne...   \n",
       "\n",
       "        id  ref_index  total_words  \\\n",
       "0  1831220        941         1661   \n",
       "1  1831220       1025         1661   \n",
       "2  1831220       1187         1661   \n",
       "3  1831220       1267         1661   \n",
       "4  1831220       1364         1661   \n",
       "\n",
       "                                    neighboring_tags  label_category  \n",
       "0  [NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...               2  \n",
       "1  [NN, ,, NNP, ,, NNP, NN, NNS, JJ, NN, NN, VBD,...               2  \n",
       "2  [,, NNP, ., JJ, JJ, NN, IN, NNS, IN, JJ, NNS, ...               2  \n",
       "3  [,, NNP, ,, NN, ., NN, IN, DT, NN, IN, JJ, NN,...               2  \n",
       "4  [JJ, NNS, IN, JJ, NN, NN, IN, NN, IN, JJ, NN, ...               2  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_dummies = pd.get_dummies(auxiliary_features.sections.apply(pd.Series).stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_features = auxiliary_features.join(section_dummies.sum(level=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>total_words</th>\n",
       "      <th>neighboring_tags</th>\n",
       "      <th>label_category</th>\n",
       "      <th>20th century</th>\n",
       "      <th>21st century</th>\n",
       "      <th>Accolades</th>\n",
       "      <th>Activities</th>\n",
       "      <th>...</th>\n",
       "      <th>Taxonomy</th>\n",
       "      <th>Terminology</th>\n",
       "      <th>Timeline</th>\n",
       "      <th>Track listing</th>\n",
       "      <th>Transfers</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Types</th>\n",
       "      <th>Uses</th>\n",
       "      <th>Work</th>\n",
       "      <th>Works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{{cite journal | author= Kenneth Cornetta | au...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>941</td>\n",
       "      <td>1661</td>\n",
       "      <td>[NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{{cite journal|last=Sorgi|first=FL|author2=Bha...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1025</td>\n",
       "      <td>1661</td>\n",
       "      <td>[NN, ,, NNP, ,, NNP, NN, NNS, JJ, NN, NN, VBD,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{{cite journal|last=Walker|first=WS|author2=Re...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1187</td>\n",
       "      <td>1661</td>\n",
       "      <td>[,, NNP, ., JJ, JJ, NN, IN, NNS, IN, JJ, NNS, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{{cite journal|last=Campbell|first=FW|author2=...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1267</td>\n",
       "      <td>1661</td>\n",
       "      <td>[,, NNP, ,, NN, ., NN, IN, DT, NN, IN, JJ, NN,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{{cite journal|last=Welsby|first=IJ|author2=Ne...</td>\n",
       "      <td>1831220</td>\n",
       "      <td>1364</td>\n",
       "      <td>1661</td>\n",
       "      <td>[JJ, NNS, IN, JJ, NN, NN, IN, NN, IN, JJ, NN, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            citation       id  ref_index  \\\n",
       "0  {{cite journal | author= Kenneth Cornetta | au...  1831220        941   \n",
       "1  {{cite journal|last=Sorgi|first=FL|author2=Bha...  1831220       1025   \n",
       "2  {{cite journal|last=Walker|first=WS|author2=Re...  1831220       1187   \n",
       "3  {{cite journal|last=Campbell|first=FW|author2=...  1831220       1267   \n",
       "4  {{cite journal|last=Welsby|first=IJ|author2=Ne...  1831220       1364   \n",
       "\n",
       "   total_words                                   neighboring_tags  \\\n",
       "0         1661  [NNP, NN, CD, NN, CD, NN, NNS, CD, JJ, VBD, JJ...   \n",
       "1         1661  [NN, ,, NNP, ,, NNP, NN, NNS, JJ, NN, NN, VBD,...   \n",
       "2         1661  [,, NNP, ., JJ, JJ, NN, IN, NNS, IN, JJ, NNS, ...   \n",
       "3         1661  [,, NNP, ,, NN, ., NN, IN, DT, NN, IN, JJ, NN,...   \n",
       "4         1661  [JJ, NNS, IN, JJ, NN, NN, IN, NN, IN, JJ, NN, ...   \n",
       "\n",
       "   label_category  20th century  21st century  Accolades  Activities  ...  \\\n",
       "0               2             0             0          0           0  ...   \n",
       "1               2             0             0          0           0  ...   \n",
       "2               2             0             0          0           0  ...   \n",
       "3               2             0             0          0           0  ...   \n",
       "4               2             0             0          0           0  ...   \n",
       "\n",
       "   Taxonomy  Terminology  Timeline  Track listing  Transfers  Treatment  \\\n",
       "0         0            0         0              0          0          0   \n",
       "1         0            0         0              0          0          0   \n",
       "2         0            0         0              0          0          0   \n",
       "3         0            0         0              0          0          0   \n",
       "4         0            0         0              0          0          0   \n",
       "\n",
       "   Types  Uses  Work  Works  \n",
       "0      0     0     0      0  \n",
       "1      0     0     0      0  \n",
       "2      0     0     0      0  \n",
       "3      0     0     0      0  \n",
       "4      0     0     0      0  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_features.drop('sections', axis=1, inplace=True)\n",
    "auxiliary_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the `type of citations` and one hot encoding it to get a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get one hot encoding of citation_type column\n",
    "# citation_type_encoding = pd.get_dummies(auxiliary_features['citation_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop column citation_type as it is now encoded and join it\n",
    "# auxiliary_features = auxiliary_features.drop('citation_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat columns of the dummies along the axis with the matching index\n",
    "# auxiliary_features = pd.concat([auxiliary_features, citation_type_encoding], axis=1)\n",
    "# auxiliary_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for the feature `total_number_of_words`, the mean and median **(since it is more robust in nature!)** are pretty high for articles which are `not` journal or books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of journal articles: 11569.929999268748\n",
      "Total median length of journal articles: 5371.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of journal articles: {}'.format( ## Journal - length is less\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 1]['total_words'].mean()))\n",
    "print('Total median length of journal articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 1]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of book articles: 6761.968974319627\n",
      "Total median length of book articles: 3097.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of book articles: {}'.format( ## Rest of the article have larger length\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 2]['total_words'].mean()))\n",
    "print('Total median length of book articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 2]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mean length of book articles: 6899.136715744908\n",
      "Total median length of book articles: 2782.0\n"
     ]
    }
   ],
   "source": [
    "print('Total mean length of book articles: {}'.format( ## Books - length is less\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 0]['total_words'].mean()))\n",
    "print('Total median length of book articles: {}'.format(\n",
    "    auxiliary_features[auxiliary_features['label_category'] == 0]['total_words'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the `neighboring_tags` and making an encoder dictionary for it\n",
    "\n",
    "To have more info about how what tag mean what: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_tag_features = dataset_with_features[['id', 'citation', 'neighboring_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NNP', 'NN', 'CD', 'NN', 'CD', 'NN', 'NNS', 'CD', 'JJ', 'VBD',\n",
       "       'JJ', 'CD', 'JJ', 'NN', 'CD', 'NN', 'NN', 'NN', 'NNP', 'CD'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features['neighboring_tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count for each POS tag so that we have an estimation as to how many are there\n",
    "tag_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_tag_features.neighboring_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L         2\n",
       "LS        3\n",
       "``      321\n",
       "`       678\n",
       "UH     1258\n",
       "Y      1463\n",
       "U      1502\n",
       "H      1502\n",
       "WP$    1543\n",
       "PDT    3661\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Considering the 10 smallest tags and checking which one does not have resemblance\n",
    "tag_counts.nsmallest(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replace `LS`, `the 2 backquotes` and the `the dollar symbol` since they do not have too much use case and do not give too much information about the context of the neighboring citation text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [00:49<00:00, 62247.91it/s] \n"
     ]
    }
   ],
   "source": [
    "OTHER_TAGS = ['LS', '``', '$']\n",
    "citation_tag_features['neighboring_tags'] = citation_tag_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: [i if i not in OTHER_TAGS else 'Others' for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `count vectorizer` to represent the `POS tags` as a vector where each element of the vector represents the count of that tag in that particular citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer() # Instantiate the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [00:05<00:00, 573030.54it/s]\n"
     ]
    }
   ],
   "source": [
    "citation_tag_features['neighboring_tags'] = citation_tag_features['neighboring_tags'].progress_apply(\n",
    "    lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_neighboring_tags = cv.fit_transform(citation_tag_features['neighboring_tags'])\n",
    "transformed_neighboring_tags = pd.DataFrame(transformed_neighboring_tags.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>citation</th>\n",
       "      <th>neighboring_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal | author= Kenneth Cornetta | au...</td>\n",
       "      <td>NNP NN CD NN CD NN NNS CD JJ VBD JJ CD JJ NN C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Sorgi|first=FL|author2=Bha...</td>\n",
       "      <td>NN , NNP , NNP NN NNS JJ NN NN VBD CD JJ JJ JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Walker|first=WS|author2=Re...</td>\n",
       "      <td>, NNP . JJ JJ NN IN NNS IN JJ NNS TO VB NNP NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Campbell|first=FW|author2=...</td>\n",
       "      <td>, NNP , NN . NN IN DT NN IN JJ NN IN JJ JJ NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Welsby|first=IJ|author2=Ne...</td>\n",
       "      <td>JJ NNS IN JJ NN NN IN NN IN JJ NN NN NN VBZ JJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           citation  \\\n",
       "0  1831220  {{cite journal | author= Kenneth Cornetta | au...   \n",
       "1  1831220  {{cite journal|last=Sorgi|first=FL|author2=Bha...   \n",
       "2  1831220  {{cite journal|last=Walker|first=WS|author2=Re...   \n",
       "3  1831220  {{cite journal|last=Campbell|first=FW|author2=...   \n",
       "4  1831220  {{cite journal|last=Welsby|first=IJ|author2=Ne...   \n",
       "\n",
       "                                    neighboring_tags  \n",
       "0  NNP NN CD NN CD NN NNS CD JJ VBD JJ CD JJ NN C...  \n",
       "1  NN , NNP , NNP NN NNS JJ NN NN VBD CD JJ JJ JJ...  \n",
       "2  , NNP . JJ JJ NN IN NNS IN JJ NNS TO VB NNP NN...  \n",
       "3  , NNP , NN . NN IN DT NN IN JJ NN IN JJ JJ NN ...  \n",
       "4  JJ NNS IN JJ NN NN IN NN IN JJ NN NN NN VBZ JJ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_tag_features = pd.concat([citation_tag_features, transformed_neighboring_tags], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>citation</th>\n",
       "      <th>cc</th>\n",
       "      <th>cd</th>\n",
       "      <th>dt</th>\n",
       "      <th>ex</th>\n",
       "      <th>fw</th>\n",
       "      <th>in</th>\n",
       "      <th>jj</th>\n",
       "      <th>jjr</th>\n",
       "      <th>...</th>\n",
       "      <th>vb</th>\n",
       "      <th>vbd</th>\n",
       "      <th>vbg</th>\n",
       "      <th>vbn</th>\n",
       "      <th>vbp</th>\n",
       "      <th>vbz</th>\n",
       "      <th>wdt</th>\n",
       "      <th>wikicode</th>\n",
       "      <th>wp</th>\n",
       "      <th>wrb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal | author= Kenneth Cornetta | au...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Sorgi|first=FL|author2=Bha...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Walker|first=WS|author2=Re...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Campbell|first=FW|author2=...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831220</td>\n",
       "      <td>{{cite journal|last=Welsby|first=IJ|author2=Ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           citation  cc  cd  dt  ex  \\\n",
       "0  1831220  {{cite journal | author= Kenneth Cornetta | au...   0   6   0   0   \n",
       "1  1831220  {{cite journal|last=Sorgi|first=FL|author2=Bha...   0   2   0   0   \n",
       "2  1831220  {{cite journal|last=Walker|first=WS|author2=Re...   0   1   0   0   \n",
       "3  1831220  {{cite journal|last=Campbell|first=FW|author2=...   0   1   1   0   \n",
       "4  1831220  {{cite journal|last=Welsby|first=IJ|author2=Ne...   0   1   0   0   \n",
       "\n",
       "   fw  in  jj  jjr  ...  vb  vbd  vbg  vbn  vbp  vbz  wdt  wikicode  wp  wrb  \n",
       "0   0   0   3    0  ...   0    1    0    0    0    0    0         0   0    0  \n",
       "1   0   0   4    0  ...   0    1    0    0    0    0    0         0   0    0  \n",
       "2   0   2   3    0  ...   1    0    0    0    0    1    0         0   0    0  \n",
       "3   0   3   3    0  ...   0    0    0    0    0    0    0         0   0    0  \n",
       "4   0   3   4    0  ...   0    0    0    0    0    1    0         0   0    0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_tag_features.drop('neighboring_tags', axis=1, inplace=True)\n",
    "citation_tag_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for the LSTM - more time sequence related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation's original text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe for preprocessing citation text\n",
    "citation_text_features = dataset_with_features[['id', 'citation', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [00:24<00:00, 122249.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the citation into a list by breaking it down into characters\n",
    "citation_text_features['characters'] = citation_text_features['citation'].progress_apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['{', 'c', 'i', 't', 'e', ' ', 'j', 'o', 'u', 'r', 'n', 'a', 'l', '|',\n",
       "       'h', '=', 'K', 'C', '2', 'W', '.', 'F', 'A', 'd', 's', 'P', 'm', 'f',\n",
       "       'v', 'p', 'y', 'b', '-', 'g', ':', 'J', 'V', 'M', '1', '9', '8', '3',\n",
       "       '7', '\\', '0', '4', '6', '/', '(', ')', '}', 'S', 'L', 'B', ',', 'H',\n",
       "       'G', 'k', 'R', 'D', 'I', '5', 'E', 'w', 'N', 'z', 'T', 'x', 'O', '&',\n",
       "       'Z', '?', 'Y', 'q', 'U', 'Q', 'X', ';', '[', ']', '_', '!', ''', '\"',\n",
       "       '<', '>', '+', '%', '#', '~', '*', '`', '^', '@', '$'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the character counts for each unique character\n",
    "char_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_text_features.characters)))\n",
    "char_counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length of the longest citation in terms of characters is: 40755\n",
      "The mean length of the longest citation in terms of characters is: 221.33374929436158\n",
      "The median length of the longest citation in terms of characters is: 185.0\n"
     ]
    }
   ],
   "source": [
    "print('The max length of the longest citation in terms of characters is: {}'.format(\n",
    "    max(citation_text_features.characters.apply(lambda x: len(x)))))\n",
    "\n",
    "print('The mean length of the longest citation in terms of characters is: {}'.format(\n",
    "    citation_text_features.characters.apply(lambda x: len(x)).mean()))\n",
    "\n",
    "print('The median length of the longest citation in terms of characters is: {}'.format(\n",
    "    citation_text_features.characters.apply(lambda x: len(x)).median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for creating a mapping between the char and the corresponding index\n",
    "char2ind = {char: i for i, char in enumerate(char_counts.index)}\n",
    "ind2char = {i: char for i, char in enumerate(char_counts.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each character into the citation to its corresponding index and store it in a list\n",
    "X_char = []\n",
    "for citation in citation_text_features.citation:\n",
    "    citation_chars = []\n",
    "    for character in citation:\n",
    "        citation_chars.append(char2ind[character])\n",
    "        \n",
    "    X_char.append(citation_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median length of the citation is 282, we have padded the input till 400 to get extra information which would be fed into the character embedding neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    X_char = pad_sequences(X_char, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [10:57<00:00, 4642.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Append the citation character list with their corresponding lists for making a dataset\n",
    "# for getting the character embeddings\n",
    "data = []\n",
    "for i in tqdm(range(len(X_char))):\n",
    "    data.append((X_char[i], int(citation_text_features.iloc[i]['label_category'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the training data and labels for further verification use\n",
    "training_data = [i[0] for i in data]\n",
    "training_labels = [i[1] for i in data]\n",
    "training_labels = [0 if i in [0,2] else 1 for i in training_labels] ## Changing it to dummy labels - identifier vs non identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to feed in the 400 character input since our median length comes out to be approximately 282 and train it on a dummy task - if the citation is scientific or not and get the embedding layer which would contain the representation for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "categorical_labels = to_categorical(training_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_embedding_model():\n",
    "    \"\"\"\n",
    "    Citation embedding generator model where the dimension of the embedding is 50.\n",
    "    \"\"\"\n",
    "    main_input = Input(shape=(400, ), name='characters')\n",
    "    # input dim is basically the vocab size\n",
    "    emb = Embedding(input_dim=95, output_dim = 300, name='citation_embedding')(main_input)\n",
    "    rnn = Bidirectional(LSTM(20))\n",
    "    x = rnn(emb)\n",
    "    de = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs = main_input, outputs = de)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "characters (InputLayer)      (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "citation_embedding (Embeddin (None, 400, 300)          28500     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 79,942\n",
      "Trainable params: 79,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and generate the summary\n",
    "model = citation_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to create batches of data so that processing is easy.\n",
    "    \n",
    "    :param: features: the features of the model.\n",
    "    :param: labels: the labels of the model.\n",
    "    :param: batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, 400))\n",
    "    batch_labels = np.zeros((batch_size, 2))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = np.random.choice(len(features), 1)[0]\n",
    "            batch_features[i] = features[index]\n",
    "            batch_labels[i] = categorical_labels[index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "30/30 [==============================] - 17s 551ms/step - loss: 0.6392 - acc: 0.6396\n",
      "Epoch 2/15\n",
      "30/30 [==============================] - 14s 479ms/step - loss: 0.2487 - acc: 0.9448\n",
      "Epoch 3/15\n",
      "30/30 [==============================] - 15s 495ms/step - loss: 0.1247 - acc: 0.9682\n",
      "Epoch 4/15\n",
      "30/30 [==============================] - 15s 493ms/step - loss: 0.0863 - acc: 0.9766\n",
      "Epoch 5/15\n",
      "30/30 [==============================] - 15s 484ms/step - loss: 0.0966 - acc: 0.9776\n",
      "Epoch 6/15\n",
      "30/30 [==============================] - 14s 468ms/step - loss: 0.0530 - acc: 0.9901\n",
      "Epoch 7/15\n",
      "30/30 [==============================] - 15s 487ms/step - loss: 0.0640 - acc: 0.9870\n",
      "Epoch 8/15\n",
      "30/30 [==============================] - 15s 484ms/step - loss: 0.0556 - acc: 0.9880\n",
      "Epoch 9/15\n",
      "30/30 [==============================] - 15s 488ms/step - loss: 0.0498 - acc: 0.9885\n",
      "Epoch 10/15\n",
      "30/30 [==============================] - 14s 477ms/step - loss: 0.0389 - acc: 0.9927\n",
      "Epoch 11/15\n",
      "30/30 [==============================] - 14s 473ms/step - loss: 0.0549 - acc: 0.9906\n",
      "Epoch 12/15\n",
      "30/30 [==============================] - 15s 492ms/step - loss: 0.0513 - acc: 0.9901\n",
      "Epoch 13/15\n",
      "30/30 [==============================] - 15s 489ms/step - loss: 0.0707 - acc: 0.9833\n",
      "Epoch 14/15\n",
      "30/30 [==============================] - 15s 487ms/step - loss: 0.1027 - acc: 0.9792\n",
      "Epoch 15/15\n",
      "30/30 [==============================] - 14s 467ms/step - loss: 0.0640 - acc: 0.9870\n"
     ]
    }
   ],
   "source": [
    "# Run the model with the data being generated by the generator with a batch size of 64\n",
    "# and number of epochs to be set to 15\n",
    "hist = model.fit_generator(generator(training_data, categorical_labels, 64), samples_per_epoch=30, nb_epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so that we can retrieve it later\n",
    "model.save('/dlabdata1/harshdee/embedding_model.h5')\n",
    "# from keras.models import load_model\n",
    "# model = load_model('/dlabdata1/harshdee/embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 300)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the `citation_embedding` layer and get the weights for each character\n",
    "citation_layer = model.get_layer('citation_embedding')\n",
    "citation_weights = citation_layer.get_weights()[0]\n",
    "citation_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.3487798e-02, -2.2857914e-02,  7.2305627e-02,  7.5458631e-02,\n",
       "        3.5729304e-02, -4.7548639e-04, -6.9378056e-02, -2.3916585e-02,\n",
       "        5.3427406e-03,  2.3297900e-03,  1.5415391e-02,  6.4162754e-02,\n",
       "       -1.7194109e-02, -8.7533649e-03,  9.1980938e-03,  2.2650823e-02,\n",
       "        2.9702580e-02, -1.0351910e-02, -8.3165988e-03,  4.4419640e-03,\n",
       "        5.4581347e-03,  3.7447099e-02,  8.4146438e-03,  1.7995673e-05,\n",
       "        6.7927120e-03, -3.2515638e-02, -2.5981069e-02, -1.0615817e-02,\n",
       "       -3.4218505e-02, -1.2037923e-02, -1.9652506e-02, -4.9414057e-03,\n",
       "       -5.4916978e-02, -4.0766921e-02,  4.5642130e-02, -4.0086053e-02,\n",
       "       -5.7412632e-02,  7.2119152e-03, -3.6836632e-02, -2.9153578e-02,\n",
       "       -6.2424974e-03, -8.4446585e-03, -4.7941878e-02, -2.1400314e-03,\n",
       "       -2.7946520e-03, -5.1635080e-03, -2.4884988e-02,  6.3802965e-02,\n",
       "       -4.4594160e-03,  1.6389739e-02,  1.5342609e-02, -1.7282961e-02,\n",
       "       -1.2013974e-02, -4.5360099e-03, -1.9862046e-02,  1.7599603e-02,\n",
       "        1.9735083e-02,  5.7086861e-03,  8.7109813e-03, -3.6289480e-02,\n",
       "       -7.4960172e-02,  6.3264966e-02, -9.2075398e-04,  2.5071471e-03,\n",
       "        1.1354457e-03,  7.5285032e-02,  3.6752462e-02,  6.0583100e-02,\n",
       "        4.6045799e-02, -3.9153814e-02,  4.0911481e-02, -2.6398038e-02,\n",
       "       -1.8777907e-02, -1.8693546e-02, -3.5392713e-02,  3.2092404e-02,\n",
       "       -2.3102339e-02, -3.6406793e-02, -4.0812530e-03, -7.0509627e-03,\n",
       "        1.9693322e-02,  5.5657964e-02, -8.2018673e-03, -6.7457749e-04,\n",
       "       -3.2703765e-02, -5.2426644e-02, -3.2936651e-02, -1.5185953e-02,\n",
       "        3.1995181e-02,  7.1764318e-03, -3.1263471e-02, -2.4604828e-05,\n",
       "       -1.0487146e-02, -5.5674583e-02, -3.3537392e-02,  5.1151831e-02,\n",
       "        2.8297089e-02, -1.6697727e-02,  4.6691850e-02,  1.7076649e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the first element of an embedding\n",
    "citation_weights[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [11:29<00:00, 4429.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Map the embedding of each character to the character in each corresponding citation and aggregate (sum)\n",
    "citation_text_features['embedding'] = citation_text_features['characters'].progress_apply(\n",
    "    lambda x: sum([citation_weights[char2ind[c]] for c in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [00:48<00:00, 63107.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the citation embeddings so that we can check for their similarity later\n",
    "citation_text_features['embedding'] = citation_text_features['embedding'].progress_apply(\n",
    "    lambda x: x/ np.linalg.norm(x, axis=0).reshape((-1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the sum of the embedding to be summed up to 1\n",
    "np.sum(np.square(citation_text_features['embedding'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Graph for citation text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just considering 20 since otherwise it will be computationally extensive\n",
    "# citation_text_and_embeddings = citation_text_features[['citation', 'embedding']][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation_text_and_embeddings['embedding'] = citation_text_and_embeddings['embedding'].progress_apply(\n",
    "#     lambda x: x[0].tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tsne_embedding_plot():\n",
    "#     labels = []\n",
    "#     tokens = []\n",
    "\n",
    "#     index = 0\n",
    "#     for row in citation_text_and_embeddings:\n",
    "#         tokens.append(row['embedding'])\n",
    "#         labels.append(str(index))\n",
    "#         index += 1\n",
    "    \n",
    "#     # Perplexity takes into account the global and local features\n",
    "#     # We are using dimensionality reduciton for 2 features and taking 2500 iterations into account\n",
    "#     tsne_model = TSNE(perplexity=40, n_components=2, n_iter=2500, random_state=0)\n",
    "#     new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     for value in new_values:\n",
    "#         x.append(value[0])\n",
    "#         y.append(value[1])\n",
    "        \n",
    "#     plt.figure(figsize=(10, 10)) \n",
    "#     for i in range(len(x)):\n",
    "#         plt.scatter(x[i],y[i])\n",
    "#         plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2),\n",
    "#                      textcoords='offset points', ha='right', va='bottom')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_embedding_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of citation embeddings which is close to each other\n",
    "# citation_text_and_embeddings[citation_text_and_embeddings.index.isin([14, 477])] # (51, 243), (0, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Similiarity of 2 citations which are very similar\n",
    "# result_similar = 1 - spatial.distance.cosine(\n",
    "#     citation_text_and_embeddings.iloc[14]['embedding'],\n",
    "#     citation_text_and_embeddings.iloc[477]['embedding']\n",
    "# )\n",
    "# result_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of citation embeddings which is NOT close to each other and are different\n",
    "# citation_text_and_embeddings[citation_text_and_embeddings.index.isin([42, 124])] # (6, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similiarity of 2 citations which are not similar\n",
    "# result_different = 1 - spatial.distance.cosine(\n",
    "#     citation_text_and_embeddings.iloc[42]['embedding'],\n",
    "#     citation_text_and_embeddings.iloc[124]['embedding']\n",
    "# )\n",
    "# result_different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText embeddings for neighboring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained embedding model on wikipedia\n",
    "model = FastText.load_fasttext_format('/dlabdata1/harshdee/wiki.en.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe for preprocessing citation words\n",
    "citation_word_features = dataset_with_features[['id', 'citation', 'neighboring_words', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [01:13<00:00, 41763.78it/s] \n"
     ]
    }
   ],
   "source": [
    "# Lowercase all the neighboring words for each of the citations\n",
    "citation_word_features['neighboring_words'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: [i.lower() for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total unique words with their respective counts in the total dataset. This is done in order to remove words which are of low frequency and will potentially act as noise to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.Series(Counter(chain.from_iterable(x for x in citation_word_features.neighboring_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 3853631\n",
      "Total number of words whose occurence is less than 4: 3568808\n",
      "Difference: 284823\n"
     ]
    }
   ],
   "source": [
    "threshold = 4\n",
    "\n",
    "x = len(word_counts)\n",
    "y = len(word_counts[word_counts <= threshold])\n",
    "print('Total words: {}\\nTotal number of words whose occurence is less than 4: {}\\nDifference: {}'.format(x, y, x-y))\n",
    "words_less_than_threshold = word_counts[word_counts <= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [07:28<00:00, 6808.36it/s] \n"
     ]
    }
   ],
   "source": [
    "# Remove the words which have a count of less than 4 and replace them with the unique <UNK> symbol\n",
    "citation_word_features['neighboring_words'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: [i if i not in words_less_than_threshold else '<UNK>' for i in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mapping between word and index or vice versa\n",
    "words = pd.Series(Counter(chain.from_iterable(x for x in citation_word_features.neighboring_words))).index\n",
    "word2ind = {w: i for i, w in enumerate(words)}\n",
    "ind2words = {i: w for i, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284824/284824 [00:18<00:00, 15644.88it/s]\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.zeros((len(word2ind), 300))\n",
    "for w in tqdm(word2ind):\n",
    "    index = word2ind[w]\n",
    "    word_embedding_matrix[index] = model.wv[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the word embedding for each word in the neighboring words, we sum the embeddings for each word together in neighboring words to get an embedding which represents the past 40 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [08:17<00:00, 6139.90it/s] \n"
     ]
    }
   ],
   "source": [
    "citation_word_features['words_embedding'] = citation_word_features['neighboring_words'].progress_apply(\n",
    "    lambda x: sum([word_embedding_matrix[word2ind[w]] for w in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the `citation_word_features` and `citation_tag_features`, so we can join them together to form `time_sequence_features` which would be fed later into the LSTM.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join time sequence features with the citations dataset\n",
    "time_sequence_features = pd.concat([citation_tag_features, citation_word_features], keys=['id', 'citation'], axis=1)\n",
    "time_sequence_features = time_sequence_features.loc[:, ~time_sequence_features.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in time features are: (3053972, 42)\n"
     ]
    }
   ],
   "source": [
    "print('Total number of samples in time features are: {}'.format(time_sequence_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation_text = auxiliary_features.iloc[:,0]\n",
    "# auxiliary_features['citation_text'] = citation_text\n",
    "# auxiliary_features.drop('citation', axis=1, inplace=True)\n",
    "# auxiliary_features.rename({'citation_text': 'citation'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3053972, 159)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join auxiliary features with the citations dataset\n",
    "citation_text_features.reset_index(drop=True, inplace=True)\n",
    "auxiliary_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "auxiliary_features = pd.concat([auxiliary_features, citation_text_features], keys=['id', 'citation'], axis=1)\n",
    "auxiliary_features = pd.concat([auxiliary_features['citation'], auxiliary_features['id']], axis=1)\n",
    "auxiliary_features = auxiliary_features.loc[:, ~auxiliary_features.columns.duplicated()]\n",
    "auxiliary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with are duplicates\n",
    "auxiliary_features.drop(['neighboring_tags', 'characters'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del word_embedding_matrix\n",
    "del citation_word_features\n",
    "del citation_text_features\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sets for `auxiliary` and `time sequence` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_with_features[['id', 'citation', 'label_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the time sequence features for the data\n",
    "time_sequence_features = pd.concat([time_sequence_features['id'], time_sequence_features['citation']], axis=1)\n",
    "time_sequence_features = pd.concat([time_sequence_features, data], keys=['id', 'citation'], axis=1)\n",
    "time_sequence_features.columns = time_sequence_features.columns.droplevel(0)\n",
    "time_sequence_features = time_sequence_features.loc[:, ~time_sequence_features.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [01:01<00:00, 49823.03it/s]\n"
     ]
    }
   ],
   "source": [
    "time_sequence_features['words_embedding'] = time_sequence_features['words_embedding'].progress_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [01:56<00:00, 26212.19it/s]\n"
     ]
    }
   ],
   "source": [
    "auxiliary_features['embedding'] = auxiliary_features['embedding'].progress_apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3053972, 3053972)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_sequence_features), len(auxiliary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training, testing and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split is done into 80-10-10 ratio so that we have more training data to train on and have validation dataset to make sure that the model is working as anticipated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "# del word_embedding_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels which will be split later\n",
    "y = auxiliary_features.loc[:, 'label_category'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mask for auxiliary dataset to get all features except the one below\n",
    "column_mask_aux = ~auxiliary_features.columns.isin(['id', 'citation', 'label_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the columns of those auxiliary features and covert them into a list\n",
    "auxiliary = auxiliary_features.loc[:, column_mask_aux].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [01:55<00:00, 26360.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Convert them into numpy array (for Keras) and stack them (if needed) as suited for the model's format\n",
    "auxiliary = [np.array(auxiliary[i][0][0] + auxiliary[i][1:]) for i in tqdm(range(len(auxiliary)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a mask for time sequences features dataset to get all features except the one below\n",
    "cols = [col for col in time_sequence_features.columns if col not in ['id', 'citation', 'label_category', 'neighboring_words']]\n",
    "stripped_tsf = time_sequence_features[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = stripped_tsf.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_structure_time_features(time_features):\n",
    "    \"\"\"\n",
    "    Concatenate features which are numbers and lists together by checking the type:\n",
    "    \n",
    "    param: time_features: the features which are considered time sequence.\n",
    "    \"\"\"\n",
    "    feature_one = np.array([i for i in time_features if isinstance(i, int)])\n",
    "    feature_two = np.array([i for i in time_features if isinstance(i, list)][0])\n",
    "    return np.array([feature_one, feature_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3053972/3053972 [01:31<00:00, 33558.31it/s]\n"
     ]
    }
   ],
   "source": [
    "time = [make_structure_time_features(time[i]) for i in tqdm(range(len(time)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating PCA to 35 components since it should be equal to the size of the vector of the tags\n",
    "pca = PCA(n_components=35)\n",
    "\n",
    "def get_reduced_words_dimension(data):\n",
    "    \"\"\"\n",
    "    Get the aggregated dataset of words and tags which has the\n",
    "    same dimensionality using PCA.\n",
    "    \n",
    "    :param: data: data which needs to be aggregated.\n",
    "    \"\"\"\n",
    "    tags = [i for i, _ in data]\n",
    "    word_embeddings = [j for _,j in data]\n",
    "    pca.fit(word_embeddings)\n",
    "    \n",
    "    word_embeddings_pca = pca.transform(word_embeddings)\n",
    "    tags = np.array(tags)\n",
    "    return np.dstack((word_embeddings_pca, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA on all the sets of data to have the dimensions of the data to be the same\n",
    "time_pca = get_reduced_words_dimension(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "del time_sequence_features\n",
    "del auxiliary_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM/Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_nn(features_aux, features_time, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Generator to create batches of data so that processing is easy.\n",
    "    \n",
    "    :param: features: the features of the model.\n",
    "    :param: labels: the labels of the model.\n",
    "    :param: batch_size: the size of the batch\n",
    "    \"\"\"\n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features_aux = np.zeros((batch_size, 453))\n",
    "    batch_features_time =  np.zeros((batch_size, 35, 2))\n",
    "    batch_labels = np.zeros((batch_size, 4))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = np.random.choice(len(features_aux), 1)[0]\n",
    "            batch_features_aux[i] = features_aux[index]\n",
    "            batch_features_time[i] = features_time[index]\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield [batch_features_time, np.asarray(batch_features_aux)], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model():\n",
    "    \"\"\"\n",
    "    Model for classifying whether a citation is scientific or not.\n",
    "    \"\"\"\n",
    "    main_input = Input(shape=(35, 2), name='time_input')\n",
    "    lstm_out = LSTM(32)(main_input)\n",
    "\n",
    "    auxiliary_input = Input(shape=(453,), name='aux_input') ## 454 without citation type, 476 with citation type\n",
    "    # Converging the auxiliary input with the LSTM output\n",
    "    x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "    # 4 fully connected layer\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "    \n",
    "    opt = Adam(0.001, decay=1e-2/5)\n",
    "    model.compile(\n",
    "        optimizer=opt, loss={'main_output': 'categorical_crossentropy'},\n",
    "        loss_weights={'main_output': 1.}, metrics=['acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "time_input (InputLayer)         (None, 35, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32)           4480        time_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 453)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 485)          0           lstm_2[0][0]                     \n",
      "                                                                 aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          62208       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            132         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 81,316\n",
      "Trainable params: 81,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the classification model\n",
    "model = classification_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ReduceLRonPlateau` so that the model does not overshoot the optimal minimum point and hence by default we start with a learning rate of 0.01 but as soon as the accuracy stop increasing the learning rate does not change which helps us converge better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert auxiliary into numpy array for indexing\n",
    "auxiliary = np.asarray(auxiliary)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_indices, x_test_indices, y_train_indices, y_test_indices = train_test_split(\n",
    "    range(auxiliary.shape[0]), range(y.shape[0]), train_size=0.9, stratify=y, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_train = auxiliary[x_train_indices]\n",
    "time_train = time_pca[x_train_indices]\n",
    "y_train = np.eye(4)[y[x_train_indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_test = auxiliary[x_test_indices]\n",
    "time_test = time_pca[x_test_indices]\n",
    "y_test = y[x_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with epochs: 5\n",
      "Epoch 1/5\n",
      "10736/10736 [==============================] - 462s 43ms/step - loss: 0.5087 - acc: 0.8744\n",
      "Epoch 2/5\n",
      "10736/10736 [==============================] - 454s 42ms/step - loss: 0.1104 - acc: 0.9683\n",
      "Epoch 3/5\n",
      "10736/10736 [==============================] - 455s 42ms/step - loss: 0.0922 - acc: 0.9743\n",
      "Epoch 4/5\n",
      "10736/10736 [==============================] - 474s 44ms/step - loss: 0.0820 - acc: 0.9772\n",
      "Epoch 5/5\n",
      "10736/10736 [==============================] - 476s 44ms/step - loss: 0.0777 - acc: 0.9786\n",
      "\n",
      "\n",
      "Doing prediction with training done for epochs: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions = []\n",
    "# for index, (train_indices, val_indices) in enumerate(skf.split(auxiliary, y)):\n",
    "#     aux_train, aux_val = auxiliary[train_indices], auxiliary[val_indices]\n",
    "#     time_train, time_val = time_pca[train_indices], time_pca[val_indices]\n",
    "#     y_train = np.eye(4)[y[train_indices]]\n",
    "#     y_val = y[val_indices]\n",
    "    \n",
    "BATCH_SIZE = 256\n",
    "print('Running model with epochs: {}'.format(EPOCHS))\n",
    "\n",
    "model = None\n",
    "model = classification_model()\n",
    "training_generator = generator_nn(aux_train, time_train, y_train, BATCH_SIZE)\n",
    "\n",
    "history_callback = model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=len(x_train_indices) // 256,\n",
    "    epochs=EPOCHS, verbose=1, shuffle=True\n",
    ")\n",
    "history_dict = history_callback.history\n",
    "json.dump(history_dict, open('/dlabdata1/harshdee/results/citation_model_loss_{}.json'.format(EPOCHS), 'w'))\n",
    "print('\\n\\nDoing prediction with training done for epochs: {}\\n'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Neural network model for epochs 5: 0.9832120708059646\n",
      "                book  entertainment  journal  newspapers  accuracy\n",
      "book           55316             24     2764           1  0.983212\n",
      "entertainment     65          60074       21          11  0.983212\n",
      "journal         2155             72    79879           3  0.983212\n",
      "newspapers         1             10        0      105002  0.983212\n"
     ]
    }
   ],
   "source": [
    "prediction_for_folds = model.predict([time_test, aux_test])\n",
    "y_pred = np.argmax(prediction_for_folds, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the Neural network model for epochs {}: {}\".format(EPOCHS, accuracy))\n",
    "\n",
    "res = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "res.index = ['book', 'entertainment', 'journal', 'newspapers']\n",
    "res.columns = ['book', 'entertainment', 'journal', 'newspapers']\n",
    "res['accuracy'] = accuracy\n",
    "res.to_csv('/dlabdata1/harshdee/results/citation_model_result_{}.csv'.format(EPOCHS))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Done with the prediction and saving model with epochs: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.save('/dlabdata1/harshdee/results/citation_model_epochs_{}.h5'.format(EPOCHS))\n",
    "json_string = model.to_json()\n",
    "with open(\"/dlabdata1/harshdee/results/citation_model_epochs_{}.json\".format(EPOCHS), \"w\") as json_file:\n",
    "    json_file.write(json_string)\n",
    "\n",
    "print('\\n\\nDone with the prediction and saving model with epochs: {}\\n'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
